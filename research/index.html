---
layout: default
title: research
---

<div class="container">

  <div class="row">
    <div class="col-md-12">
      <div class="page-header">
        <h1>research</h1>
      </div>
    </div>
  </div><!--/ .row -->

  <div class="row">

  </div><!--/ .row -->

  <div class="row">
    <div class="col-md-12">

      <div class="page-header">
        <h2>artifacts</h2>
      </div>

      <ul class="media-list">
        <li class="media">
          <img class="media-object pull-left" src="img/hotspotizer.jpg" alt="Hotspotizer" width="160px">
          <div class="media-body">
            <h4 class="media-heading">
              <strong>Hotspotizer</strong>
              <br>
              Koç University Design Lab (2013&ndash;2014)
            </h4>
            <p>
              Hotspotizer allows users without computer programming skills to design, visualize, save and recall sets of custom full-body gestures for the Kinect sensor. These gestures are mapped to system-wide keyboard commands which can be used to control any application. Hotspotizer is centered around a novel, easy-to-use graphical interface based on cubic voxels. It is built as an end-to-end, standalone Windows app intended for lay users. A working release of the application and the source code are on GitHub.
            </p>
            <p>
              <a class="btn btn-primary btn-xs" target="_blank" href="http://github.com/mbaytas/hotspotizer"><i class="fa fa-github"></i> GitHub</a>
              <a class="btn btn-info btn-xs" target="_blank" href="#" data-toggle="modal" data-target="#relpubs-hotspotizer"><i class="fa fa-graduation-cap"></i> Related Publications</a>
            </p>
            <div class="modal fade" id="relpubs-hotspotizer" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
              <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    <h4 class="modal-title" id="myModalLabel">Academic Publications Related to <strong>Hotspotizer</strong></h4>
                  </div>
                  <div class="modal-body">
                    <p>Baytaş, M.A., Yemez, Y., and Özcan, O. (2014) Hotspotizer: end-user authoring of mid-air gestural interactions. In <em>NordiCHI '14</em>.</p>
                    <p>Baytaş, M.A. (2014) End-User Authoring of Mid-Air Gestural Interactions (MA Thesis).</p>
                    <p>Baytaş, M.A., Yemez, Y., and Özcan, O. (2014) User Interface Paradigms for Visually Authoring Mid-Air Gestures: A Survey and a Provocation. In <em>EGMI '14</em>.</p>
                    <p>Ünlüer, A.A., and Özcan, O. (2013) Mime-based creative drama implementations for exploring gestural interaction. <em>Digital Creativity</em>, 24, 4.</p>
                    <p>Ünlüer, A.A., and Özcan, O. (2012) Current Problems in Design Education on Natural User Interface Using Creative Drama Technique. In <em>DTRS 2012</em>.</p>
                  </div>
                </div>
              </div>
            </div><!-- .modal -->
          </div>
        </li>
        <li class="media">
          <img class="media-object pull-left" src="img/biosensor.png" alt="MEMS Biosensor Readout Automation UI" width="160px">
          <div class="media-body">
            <h4 class="media-heading">
              <strong>Readout Automation and GUI for an Experimental &mu;-cantilever-based MEMS Biosensor</strong>
              <br>
              Koç University Optical Microsystems Laboratory (2011)
            </h4>
            <p>
              OML's multi-analyte MEMS biosensor uses an array of coated &mu;-cantilevers that shift their resonant frequencies upon analyte mass accretion, allowing the detection of analyte concentrations. The cantilevers are magnetically actuated and their resonant frequencies are observed via interferometric optical readout. The remote and wireless chip is intended for use within a portable device. I designed and implemented a custom GUI and mechanism for setting up characterization experiments by directly manipulating the position of the chip relative to the laser beam. The system then traverses the &mu;-cantilever array and collects data without supervision.
            </p>
            <p>
              <a class="btn btn-primary btn-xs" target="_blank" href="http://mems.ku.edu.tr/?page_id=747"><i class="fa fa-external-link-square"></i> Project Page</a>
              <a class="btn btn-info btn-xs" target="_blank" href="#" data-toggle="modal" data-target="#relpubs-biosensor"><i class="fa fa-graduation-cap"></i> Related Publications</a>
            </p>
            <div class="modal fade" id="relpubs-biosensor" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
              <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    <h4 class="modal-title" id="myModalLabel">Academic Publications Related to <strong>MEMS Biosensor for Diagnostics</strong></h4>
                  </div>
                  <div class="modal-body">
                    <p>O. Cakmak, N. Kilinc, E.Ermek, A. Mostafazadeh, C. Elbuken, G.G. Yaralioglu, H. Urey “LoC Sensor Array Platform for Real-Time Coagulation Measurements ” accepted to IEEE MEMS 2014.</p>
                    <p>O. Cakmak, N. Kilinc, E.Ermek, G.G. Yaralioglu, H. Urey “MEMS Based Blood Plasma Viscosity Sensor Without Electrical Connections” in IEEE Sensors 2013,  Baltimore, Maryland, USA, November 2013.</p>
                    <p>N Kilinc, O Cakmak, A Kosemen, E Ermek, S Ozturk, Y Yerli, ZZ Ozturk, H Urey, “A Voc Sensor Based on Micromechanical Cantilever Functionalized with ZnO Nanorods”  The 17th International Conference on Miniaturized Systems for Chemistry and Life Sciences (MicroTas 2013), Freiburg, Germany, October 2013.</p>
                    <p>Cakmak O, Elbuken C., Ermek E., Mostafazadeh A., Alaca B.E., Urey H., “Microcantilever Based Disposable Viscosity Sensor for Serum and Blood Plasma Measurements” Methods, 63 (2013) 225-232.</p>
                    <p>O. Çakmak, Ç. Elbüken, E. Ermek, S. Bulut, Y. Kılınç, I. Barış, H. Kavaklı, E. Alaca, H. Ürey, ” MEMS biosensor for blood plasma viscosity measurements” New Biotechnology, 29, Supplement  S162-S163, 2012. (also published as Proceedings of the European Congress on Biotechnolgy Conference)</p>
                    <p>Erman Timurdogan, Natali Ozber, Sezin Nargul, Serhat Yavuz, M. Salih Kilic, I. Halil Kavakli, Hakan         Urey, and B. Erdem Alaca, &#8221; Detection of human K-<wbr />opioid antibody using microresonators with integrated    optical readout,&#8221;  Biosensors and Bioelectronics, Vol. 26, pp. 195-<wbr />201, 2010</p>
                    <p>A. Ozturk, H. I. Ocakli, N. Ozber, H. Kavakli, H. Urey, E. Alaca, &#8220;A magnetically actuated resonant mass     sensor with integrated optical readout,&#8221;  Phot. Tech.. Lett., Vol. 20, 1905-<wbr />1907, 2008</p>
                  </div>
                </div>
              </div>
            </div><!-- .modal -->
          </div>
        </li>
        </li>
        <li class="media">
          <img class="media-object pull-left" src="img/laser.jpg" alt="Hotspotizer" width="160px">
          <div class="media-body">
            <h4 class="media-heading">
              <strong>Workstation for Laser Machining and Additive Manufacturing</strong>
              <br>
              Koç University Manufacturing & Automation Research Center (2010)
            </h4>
            <p>
              MARC's experimental laser manufacturing workstation is a versatile machine with marking, cutting, engraving and powder sintering capabilities. The workstation utilizes a 10.6 &mu;m CO<sub>2</sub> laser coupled to a 3-axis CNC positioning system, as well as a galvo-driven 1064 nm Nd:YAG laser. The machine is    controlled via a custom UI and back-end developed in MATLAB, and an Arduino. The software, which I partly developed, also supports toolpath and G-code generation from STL models. The Arduino and peripheral electronics, which I partly designed and realized, receive user input from the software and control the machine's industrial CO<sub>2</sub> laser, AC servos, galvanometric scanner and powder sintering bed mechanism. The chassis is designed, mechanically analyzed, fabricated and hand-assembled by a team of three.
            </p>
            <p>
              <a class="btn btn-primary btn-xs" target="_blank" href="http://marc.ku.edu.tr/projects"><i class="fa fa-external-link-square"></i> Project Page</a>
              <a class="btn btn-info btn-xs" target="_blank" href="#" data-toggle="modal" data-target="#relpubs-laser"><i class="fa fa-graduation-cap"></i> Related Publications</a>
            </p>
            <div class="modal fade" id="relpubs-laser" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
              <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    <h4 class="modal-title" id="myModalLabel">Academic Publications Related to <strong>Workstation for Laser Machining and Additive Manufacturing</strong></h4>
                  </div>
                  <div class="modal-body">
                    <p>Bank, H.S., and Lazoglu, I. (2012) Development of a New Hybrid Laser Workstation for Additive Manufacturing and Laser Machining. In <em>UMTIK '12</em>.</p>
                  </div>
                </div>
              </div>
            </div><!-- .modal -->
          </div>
        </li>
      </ul>

    </div>
  </div><!--/ .row -->

  <div class="row">
    <div class="col-md-12">

      <div class="page-header">
        <h2>publications</h2>
      </div>

      <ul class="media-list">

        <li class="media">
          <img class="media-object pull-left" src="img/2016-perception.png" alt="The Perception of Live-sequenced Electronic Music via Hearing and Sight" width="160px">
          <div class="media-body">
            <h4 class="media-heading">
              Mehmet Aydın Baytaş, Tilbe Göksun and Oğuzhan Özcan (2016)<br>
              <strong>The Perception of Live-sequenced Electronic Music via Hearing and Sight</strong><br>
              in <em>Proceedings of the 2016 International Conference on New Interfaces for Musical Expression (NIME 2016) (forthcoming)</em>
            </h4>
            <p>
              In this paper, we investigate how watching a live-sequenced electronic music performance, compared to merely hearing the music, contributes to spectators’ experiences of tension. We also explore the role of the performers’ "effective" and "ancillary" gestures in conveying tension, when they can be seen. To this end, we conducted an experiment where 30 participants heard, saw, or both heard and saw a live-sequenced techno music performance recording while they produced continuous judgments on their experience of tension. Eye tracking data was also recorded from participants who saw the visuals, to reveal aspects of the performance that influenced their tension judgments. We analysed the data to explore how auditory and visual components and the performer’s movements contribute to spectators’ experience of tension. Our results show that their perception of emotional intensity is consistent across hearing and sight, suggesting that gestures in “non-instrumental” live-sequencing can be a medium for expressive performance.
            </p>
            <p>
            </p>
            <div class="modal fade" id="bibtex-2016-NIME" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
              <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    <h4 class="modal-title" id="myModalLabel">BibTeX Entry for <strong>The Perception of Live-sequenced Electronic Music via Hearing and Sight</strong></h4>
                  </div>
                  <div class="modal-body">
                    <pre>@inproceedings{Baytas:2016:NIME,
}</pre>
                  </div>
                </div>
              </div>
            </div><!-- .modal -->
          </div>
        </li>

        <li class="media">
          <img class="media-object pull-left" src="img/2014-hotspotizer-end-user.png" alt="Hotspotizer: End-User Authoring of Mid-Air Gestural Interactions" width="160px">
          <div class="media-body">
            <h4 class="media-heading">
              Mehmet Aydın Baytaş, Yücel Yemez and Oğuzhan Özcan (2014)<br>
              <strong>Hotspotizer: End-User Authoring of Mid-Air Gestural Interactions</strong><br>
              in <em>Proceedings of the 8th Nordic Conference on Human-Computer Interaction (NordiCHI '14)</em>
            </h4>
            <p>
              Drawing from a user-centered design process and guidelines derived from the literature, we developed a paradigm based on space discretization for declaratively authoring mid-air gestures and implemented it in Hotspotizer, an end-to-end toolkit for mapping custom gestures to keyboard commands. Our implementation empowers diverse user populations – including end-users without domain expertise – to develop custom gestural interfaces within minutes, for use with arbitrary applications.
            </p>
            <p>
              <a class="btn btn-primary btn-xs" target="_blank" href="http://dl.acm.org/citation.cfm?id=2639189"><i class="fa fa-book"></i> Volume</a>
              <a class="btn btn-primary btn-xs" target="_blank" href="http://dl.acm.org/citation.cfm?id=2639255"><i class="fa fa-file-pdf-o"></i> PDF</a>
              <a class="btn btn-info btn-xs" target="_blank" href="#" data-toggle="modal" data-target="#bibtex-2014-NordiCHI">BibTeX</a>
            </p>
            <div class="modal fade" id="bibtex-2014-NordiCHI" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
              <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    <h4 class="modal-title" id="myModalLabel">BibTeX Entry for <strong>Hotspotizer: End-User Authoring of Mid-Air Gestural Interactions</strong></h4>
                  </div>
                  <div class="modal-body">
                    <pre>@inproceedings{Baytas:2014:NordiCHI,
author = {Bayta\c{s}, Mehmet Ayd\in and Yemez, Y\"{u}cel and \"{O}zcan, O\u{g}uzhan},
 title = {Hotspotizer: End-user Authoring of Mid-air Gestural Interactions},
 booktitle = {Proceedings of the 8th Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational},
 series = {NordiCHI '14},
 year = {2014},
 location = {Helsinki, Finland},
 pages = {677--686},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2639189.2639255},
 doi = {10.1145/2639189.2639255},
 acmid = {2639255},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {end-user development, gestural interaction, gesture authoring, hotspotizer, interface prototyping, visual programming},
}</pre>
                  </div>
                </div>
              </div>
            </div><!-- .modal -->
          </div>
        </li>

        <li class="media">
          <img class="media-object pull-left" src="img/2014-thesis.png" alt="End-User Authoring of Mid-Air Gestural Interactions" width="160px">
          <div class="media-body">
            <h4 class="media-heading">
              Mehmet Aydın Baytaş (2014)<br>
              <strong>End-User Authoring of Mid-Air Gestural Interactions</strong><br>
              master's thesis submitted to the Koç University Graduate School of Social Sciences and Humanities
            </h4>
            <p>
              Devices that sense the alignment and motion of human limbs via computer vision have recently become a commodity; enabling a variety of novel user interfaces that use human gesture as the main input modality. The design and development of these interfaces requires programming tools that support the representation, creation and manipulation of information on human body gestures. Following concerns such as usability and physical differences among individuals, these tools should ideally target end-users and designers as well as professional software developers. This thesis documents the design, development, deployment and evaluation of a software application to support gesture authoring by end-users for skeletal tracking vision-based input devices. The software enables end-users without programming experience to introduce gesture control to computing applications that serve their own goals; and provides developers and designers of gestural interfaces with a rapid prototyping tool that can be used to experientially evaluate designs.
            </p>
            <p>
              <a class="btn btn-primary btn-xs" target="_blank" href="https://github.com/mbaytas/ma-thesis"><i class="fa fa-github"></i> GitHub</a>
              <a class="btn btn-primary btn-xs" target="_blank" href="https://github.com/mbaytas/ma-thesis/releases"><i class="fa fa-file-pdf-o"></i> PDF</a>
              <a class="btn btn-info btn-xs" target="_blank" href="#" data-toggle="modal" data-target="#bibtex-2014-thesis">BibTeX</a>
            </p>
            <div class="modal fade" id="bibtex-2014-thesis" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
              <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    <h4 class="modal-title" id="myModalLabel">BibTeX Entry for <strong>End-User Authoring of Mid-Air Gestural Interactions</strong></h4>
                  </div>
                  <div class="modal-body">
                    <pre>@mastersthesis{Baytas:2014:thesis,
author = {Bayta\c{s}, Mehmet Ayd\in},
year = {2014},
title = {End-User Authoring of Mid-Air Gestural Interactions},
institution = {Koç University},
keywords = {Hotspotizer, gestural interaction, gesture authoring, visual programming, end-user development, interface prototyping, mid-air gestures, perceptual interaction, Kinect}
}</pre>
                  </div>
                </div>
              </div>
            </div><!-- .modal -->
          </div>
        </li>

        <li class="media">
          <img class="media-object pull-left" src="img/2014-user-interface-paradigms.jpg" alt="User Interface Paradigms for Visually Authoring Mid-air Gestures: A Survey and a Provocation" width="160px">
          <div class="media-body">
            <h4 class="media-heading">
              Mehmet Aydın Baytaş, Yücel Yemez and Oğuzhan Özcan (2014)<br>
              <strong>User Interface Paradigms for Visually Authoring Mid-Air Gestures: A Survey and a Provocation</strong><br>
              in <em>Proceedings of the Workshop on Engineering Gestures for Multimodal Interfaces (EGMI 2014)</em>
            </h4>
            <p>
              Gesture authoring tools enable the rapid and experiential prototyping of gesture-based interfaces. We survey visual authoring tools for mid-air gestures and identify three paradigms used for representing and manipulating gesture information: graphs, visual markup languages and timelines. We examine the strengths and limitations of these approaches and we propose a novel paradigm to authoring location-based mid-air gestures based on space discretization.
            </p>
            <p>
              <a class="btn btn-primary btn-xs" target="_blank" href="http://ceur-ws.org/Vol-1190/"><i class="fa fa-book"></i> Volume</a>
              <a class="btn btn-primary btn-xs" target="_blank" href="http://ceur-ws.org/Vol-1190/paper2.pdf"><i class="fa fa-file-pdf-o"></i> PDF</a>
              <a class="btn btn-info btn-xs" target="_blank" href="#" data-toggle="modal" data-target="#bibtex-2014-egmi">BibTeX</a>
            </p>
            <div class="modal fade" id="bibtex-2014-egmi" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
              <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    <h4 class="modal-title" id="myModalLabel">BibTeX Entry for <strong>User Interface Paradigms for Visually Authoring Mid-Air Gestures: A Survey and a Provocation</strong></h4>
                  </div>
                  <div class="modal-body">
                    <pre>@inproceedings{Baytas:2014:EGMI,
author = {Bayta\c{s}, Mehmet Ayd\in and Yemez, Y\"{u}cel and \"{O}zcan, O\u{g}uzhan},
year = {2014},
title = {User Interface Paradigms for Visually Authoring Mid-Air Gestures: A Survey and a Provocation},
booktitle = {Proceedings of the Workshop on Engineering Gestures for Multimodal Interfaces},
series = {EGMI 2014},
pages = {8--14},
editor = {Echthler, Florian and Hoste, Lode and Kammer, Dietrich and Signer, Beat and Vanacken, Davy},
publisher = {Sun SITE Central Europe (CEUR)},
address = {Aachen, DE},
keywords = {gestural interaction, gesture authoring, visual programming, interface prototyping},
}</pre>
                  </div>
                </div>
              </div>
            </div><!-- .modal -->
          </div>
        </li>

        <li class="media">
          <img class="media-object pull-left" src="img/2012-rethinking-spherical-media.jpg" alt="Rethinking Spherical Media Surfaces by Re-reading Ancient Greek Vases" width="160px">
          <div class="media-body">
            <h4 class="media-heading">
              Oğuzhan Özcan, Ayça Ünlüer, Mehmet Aydın Baytaş and Barış Serim (2012)<br>
              <strong>Rethinking Spherical Media Surfaces by Re-reading Ancient Greek Vases</strong><br>
              paper presented at the <em>ITS'12</em> workshop <em>Beyond Flat Displays: Towards Shaped and Deformable Interactive Surfaces</em>
            </h4>
            <p>
              In this paper, we propose re-reading of past artifacts and traditions as a possible way to inspire the design of future media on non-flat displays. As an example, we illustrate how different narrative typologies found in ancient Greek vases, circular story reading, bottom-up time reading, abstract and realistic contrast reading and reading in alignment, can yield alternatives to interactive content specific to spherical media. We conclude by pointing out design considerations regarding the composition of graphic elements on spherical surfaces.
            </p>
            <p>
              <a class="btn btn-primary btn-xs" target="_blank" href="http://displayworkshop.media.mit.edu/ITS2012/papers.html"><i class="fa fa-book"></i> Volume</a>
              <a class="btn btn-primary btn-xs" target="_blank" href="http://displayworkshop.media.mit.edu/ITS2012/downloads/paper-Ozcan.pdf"><i class="fa fa-file-pdf-o"></i> PDF</a>
              <a class="btn btn-info btn-xs" target="_blank" href="#" data-toggle="modal" data-target="#bibtex-2012-its">BibTeX</a>
            </p>
            <div class="modal fade" id="bibtex-2012-its" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
              <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    <h4 class="modal-title" id="myModalLabel">BibTeX Entry for <strong>Rethinking Spherical Media Surfaces by Re-reading Ancient Greek Vases</strong></h4>
                  </div>
                  <div class="modal-body">
                    <pre>@inproceedings{Ozcan:2012,
author = {\"{O}zcan, O\u{g}uzhan and \”{U}nl\"{u}er, Ay\c{c}a and Bayta\c{s}, Mehmet Ayd\in and Serim, Bar\i\c{s}},
year = {2012},
title = {Rethinking Spherical Media Surfaces by Re-reading Ancient Greek Vases},
booktitle = {Beyond Flat Displays: Towards Shaped and Deformable Interactive Surfaces},
keywords = {spherical displays, re-reading, story-telling},
}</pre>
                  </div>
                </div>
              </div>
            </div><!-- .modal -->
          </div>
        </li>

      </ul><!--/ .media-list -->

    </div>
  </div><!--/ .row -->

  <div class="row">
    <div class="col-md-12">

      <div class="page-header">
        <h2>reviewer for</h2>
      </div>
      <ul class="list-unstyled">
        <li>Nordic Conference on Human-Computer Interaction (NordiCHI)</li>
        <li>ACM SIGCHI Symposium on Engineering Interactive Computing Systems (EICS)</li>
      </ul>

    </div>
  </div><!--/ .row -->
  
    <div class="row">
    <div class="col-md-12">

      <div class="page-header">
        <h2>awards and scholarships</h2>
      </div>
      <ul class="list-unstyled">
        <li>Koç University GSSSH (Graduate School of Social Sciences and Humanities) Fellowship (2014&ndash;2018)</li>
        <li>TÜBİTAK 1001 Scholarship (2012&ndash;2014, grant #112E056)</li>
        <li>Vehbi Koç Scholarship (2010)</li>
        <li>Koç University Merit Scholarship (2007&ndash;2012)</li>
        <li>Higher Education Loans and Housing Agency (Yüksek Öğrenim Kredi ve Yurtlar Kurumu) Scholarship (2007&ndash;2011)</li>
      </ul>

    </div>
  </div><!--/ .row -->
  
  <div class="row">
    <div class="col-md-12">

      <div class="page-header">
        <h2>externally funded projects</h2>
      </div>
      
      <h3>as coordinator</h3>

      <ul class="list-unstyled">
        <li>
          Arçelik A.Ş. (2015-2018): "KUAR: Koç University &ndash; Arçelik Research Center for Creative Industries" (&#8378;8.500.000)
        </li>
      </ul>
      
      <h3>as team member with administrative responsibilities</h3>

      <ul class="list-unstyled">
        <li>
          FP7-PEOPLE-2012-IAPP (2013-2014): “NaMoCap: Natural Motion Capture Process for Creative Industries” (Grant #324333, &euro;658.000)
        </li>
        <li>
          TÜBİTAK 1001 (2012-2014): “Specifications on a Design Education Methodology for Gestural Interface Design" (Grant #112E056, &#8378;188.000)
        </li>
      </ul>
      
      <h3>as researcher</h3>
      <ul class="list-unstyled">
        <li>
          KoçSistem (2012-2013): "Monitoring & Control Center Touchless Gesture 3D Interactive HMI” (&#8378;110.000)
        </li>
      </ul>

    </div>
  </div><!--/ .row -->

</div><!--/ .container -->

<style>
  ul.list-labels li {
    margin-bottom: 15px;
  }
  ul.list-labels li span.label {
    font-size: 15px;
    font-weight: normal;
  }
  .pre-bibtex {
    display: none;
  }
</style>

<script>
  var closeBibtex = function() {
    $(".pre-bibtex").slideUp("slow");
    $(":not(.pre-bibtex)").unbind( "click", closeBibtex);
  };
  $(".btn-bibtex").click(function(event){
    var bibtexId = $(this).attr('data-bibtex');
    $("#pre-bibtex-"+bibtexId).slideDown("slow", function(){
      $(":not(.pre-bibtex)").bind("click", closeBibtex);
    });
  });
</script>
